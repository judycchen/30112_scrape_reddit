{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b18703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21f39d8",
   "metadata": {},
   "source": [
    "### Loading scraped raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fe18d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load standardize metadata\n",
    "\n",
    "# Base folder where the Parquet file lives\n",
    "BASE_PATH = Path(\"/Users/apple/Desktop/30112_python/Scrape_Reddit\")\n",
    "\n",
    "# Parquet file produced by the Arctic Shift scraping script (TEST mode)\n",
    "parquet_path = BASE_PATH / \"scraped_data\" / \"reddit_inflation_2020_2025_posts_and_comments_TEST.parquet\"\n",
    "\n",
    "# Sanity check: make sure the file exists before loading\n",
    "print(\"Parquet path:\", parquet_path)\n",
    "print(\"Exists?\", parquet_path.exists())\n",
    "\n",
    "# Load the full Reddit dataset (posts + comments)\n",
    "df = pd.read_parquet(parquet_path)\n",
    "print(\"Original shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bc7603",
   "metadata": {},
   "source": [
    "### Cleaning missing and unmeaningful data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11452d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the columns we actually need for analysis.\n",
    "# If some are missing (e.g., num_comments on comments), we just keep the ones that exist.\n",
    "EXPECTED_COLS = [\n",
    "    \"type\",          # \"submission\" or \"comment\"\n",
    "    \"id\",\n",
    "    \"link_id\",\n",
    "    \"parent_id\",\n",
    "    \"subreddit\",\n",
    "    \"title\",\n",
    "    \"body\",          # main text for comments, selftext for posts\n",
    "    \"created_utc\",   # timestamp (epoch seconds)\n",
    "    \"score\",\n",
    "    \"num_comments\",  # only meaningful for submissions\n",
    "]\n",
    "\n",
    "keep_cols = [c for c in EXPECTED_COLS if c in df.columns]\n",
    "df = df[keep_cols].copy()\n",
    "print(\"After column selection:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694d81e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXED: Properly convert created_utc (epoch seconds) into a timezone-aware datetime.\n",
    "# Check data type first\n",
    "print(\"Original created_utc dtype:\", df[\"created_utc\"].dtype)\n",
    "print(\"Sample values before conversion:\")\n",
    "print(df[\"created_utc\"].head())\n",
    "\n",
    "# Convert from Unix timestamp (seconds since epoch) to datetime\n",
    "# The 'unit' parameter is critical - it must be 's' for seconds\n",
    "df[\"created_utc\"] = pd.to_datetime(df[\"created_utc\"], unit=\"s\", utc=True)\n",
    "\n",
    "# Verify conversion worked\n",
    "print(\"\\nAfter conversion:\")\n",
    "print(\"New created_utc dtype:\", df[\"created_utc\"].dtype)\n",
    "print(\"Sample datetime values:\")\n",
    "print(df[\"created_utc\"].head())\n",
    "\n",
    "# Add year/month/date columns for grouping later.\n",
    "df[\"date\"] = df[\"created_utc\"].dt.date\n",
    "df[\"year\"] = df[\"created_utc\"].dt.year\n",
    "df[\"month\"] = df[\"created_utc\"].dt.month\n",
    "\n",
    "# Verify year range\n",
    "print(\"\\nYear distribution:\")\n",
    "print(df[\"year\"].value_counts().sort_index())\n",
    "\n",
    "df[[\"created_utc\", \"date\", \"year\", \"month\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8dc50f",
   "metadata": {},
   "source": [
    "### Text cleaning & Preprocessing\n",
    "(Organizing & Tokenizing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5fdf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Run these once in a fresh environment (comment them out afterwards).\n",
    "# They download NLTK data files to your machine.\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# English stopword list (e.g., \"the\", \"and\", \"is\")\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Translation table to remove punctuation characters\n",
    "PUNCT_TABLE = str.maketrans(\"\", \"\", string.punctuation)\n",
    "\n",
    "def clean_text_basic(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Basic text cleaner:\n",
    "    - Handle missing values\n",
    "    - Remove URLs\n",
    "    - Lowercase text\n",
    "    - Remove punctuation\n",
    "    - Collapse extra whitespace\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # Remove URLs (http..., www...)\n",
    "    text = re.sub(r\"http\\S+|www\\.\\S+\", \" \", text)\n",
    "    # Lowercase everything\n",
    "    text = text.lower()\n",
    "    # Remove punctuation characters\n",
    "    text = text.translate(PUNCT_TABLE)\n",
    "    # Replace multiple spaces/newlines with a single space\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def tokenize_and_remove_stopwords(text: str):\n",
    "    \"\"\"\n",
    "    Tokenize a cleaned text string, then:\n",
    "    - Keep only alphabetic tokens (no numbers, no leftover punctuation)\n",
    "    - Drop stopwords like \"the\", \"and\", etc.\n",
    "    Returns a list of tokens.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [t for t in tokens if t.isalpha() and t not in STOPWORDS]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7dfac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine title + body into a single raw_text field so we always\n",
    "# have all the context for a post or comment in one place.\n",
    "# (For comments, title will be empty; for posts, body is the selftext.)\n",
    "df[\"raw_text\"] = (\n",
    "    df[[\"title\", \"body\"]]\n",
    "    .fillna(\"\")         # replace NaN with empty strings\n",
    "    .agg(\" \".join, axis=1)  # join title and body with a space\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "# Apply the cleaner to get a normalized string version.\n",
    "df[\"clean_text\"] = df[\"raw_text\"].apply(clean_text_basic)\n",
    "\n",
    "# Tokenize into a list of words (no stopwords, no punctuation).\n",
    "df[\"tokens\"] = df[\"clean_text\"].apply(tokenize_and_remove_stopwords)\n",
    "\n",
    "# Quick peek at the processed columns to make sure things look reasonable.\n",
    "df[[\"type\", \"subreddit\", \"created_utc\", \"raw_text\", \"clean_text\", \"tokens\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcde1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as Parquet (recommended for speed/size)\n",
    "\n",
    "out_path = BASE_PATH / \"scraped_data\" / \"reddit_inflation_2020_2025_posts_and_comments_TEST_clean.parquet\"\n",
    "\n",
    "# Create folders if they don't exist\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df.to_parquet(out_path, engine=\"pyarrow\", index=False)\n",
    "print(\"Saved cleaned data to:\", out_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipynb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
