{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de887214",
   "metadata": {},
   "source": [
    "### Below is a concise Python template that:\n",
    "1. Loops over my subreddit list.\n",
    "2. Downloads both submissions and comments for 2020â€“2025 from Arctic Shift's API.\n",
    "3. Writes one raw file per subreddit.\n",
    "4. Merges everything and filters by your keyword list into a final dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022be7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ddf2ac",
   "metadata": {},
   "source": [
    "## Obtain base_url and search UIs\n",
    "from Arctic Shift Github README (https://github.com/ArthurHeitmann/arctic_shift/blob/master/api/README.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673d7941",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL_POSTS = \"https://arctic-shift.photon-reddit.com/api/posts/search\"\n",
    "BASE_URL_COMMENTS = \"https://arctic-shift.photon-reddit.com/api/comments/search\"\n",
    "\n",
    "# True = single-subreddit test; False = full run\n",
    "TEST_MODE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c415d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search parameters\n",
    "\n",
    "# Core topics\n",
    "ALL_SUBREDDITS = [\n",
    "    \"economy\",\n",
    "    \"Economics\",\n",
    "    \"AskEconomics\",\n",
    "    \"personalfinance\",\n",
    "    \"povertyfinance\",\n",
    "    \"financialindependence\",\n",
    "    \"PersonalFinanceCanada\",\n",
    "    \"investing\",\n",
    "    \"stocks\",\n",
    "    \"RealEstate\",\n",
    "    \"RealEstateInvesting\",\n",
    "    \"personalfinancebanking\",\n",
    "    \"creditcards\",\n",
    "]\n",
    "\n",
    "# Search period (from...to...)\n",
    "# FIXED: Changed to full 2020-2025 range for TEST mode\n",
    "AFTER_DATE = \"2020-01-01\"\n",
    "BEFORE_DATE = \"2025-12-31\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5290f1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CHANGE TEST_MODE TO FALSE FOR FORMAL SCRAPING\n",
    "TEST_SUBREDDIT = \"personalfinance\"\n",
    "\n",
    "if TEST_MODE:\n",
    "    SUBREDDITS = [TEST_SUBREDDIT]\n",
    "else:\n",
    "    SUBREDDITS = ALL_SUBREDDITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0839afa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base path\n",
    "BASE_PATH = Path(\"/Users/apple/Desktop/30112_python/Scrape_Reddit/scraped_data\")\n",
    "\n",
    "# Raw output folder inside that path\n",
    "OUT_DIR = BASE_PATH / \"arctic_raw\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2f8663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keywords to search for if needed later\n",
    "\n",
    "KEYWORDS = [\n",
    "    # Core inflation/econ\n",
    "    \"inflation\",\n",
    "    \"cost of living\",\n",
    "    \"cost-of-living\",\n",
    "    \"high cost of living\",\n",
    "    \"living costs\",\n",
    "    \"price increases\",\n",
    "    \"rising prices\",\n",
    "    \"cpi\",\n",
    "    \"consumer price index\",\n",
    "    \"interest rates\",\n",
    "    \"mortgage rates\",\n",
    "    \"fed\",\n",
    "    \"federal reserve\",\n",
    "    \"rate hike\",\n",
    "    \"rate hikes\",\n",
    "    \"rate increase\",\n",
    "    \"rate increases\",\n",
    "    # Everyday expenses\n",
    "    \"gas prices\",\n",
    "    \"gas price\",\n",
    "    \"grocery prices\",\n",
    "    \"grocery bill\",\n",
    "    \"food prices\",\n",
    "    \"food bill\",\n",
    "    \"rent increase\",\n",
    "    \"rent hike\",\n",
    "    \"higher rent\",\n",
    "    \"rent is too high\",\n",
    "    \"housing costs\",\n",
    "    \"housing affordability\",\n",
    "    \"property taxes\",\n",
    "    \"electric bill\",\n",
    "    \"electricity bill\",\n",
    "    \"energy bill\",\n",
    "    \"heating bill\",\n",
    "    \"gas bill\",\n",
    "    \"utility bills\",\n",
    "    \"utilities\",\n",
    "    # Income / strain\n",
    "    \"wage stagnation\",\n",
    "    \"wages not keeping up\",\n",
    "    \"real wages\",\n",
    "    \"paycheck to paycheck\",\n",
    "    \"making ends meet\",\n",
    "    \"can't afford\",\n",
    "    \"cannot afford\",\n",
    "]\n",
    "KEYWORDS_LOWER = [k.lower() for k in KEYWORDS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d019dfe0",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46e8135",
   "metadata": {},
   "source": [
    ">Fetch posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1314b462",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_posts(subreddit: str, after: str, before: str, outfile: Path):\n",
    "    current_before = before\n",
    "\n",
    "    with outfile.open(\"w\", encoding=\"utf-8\") as f_out:\n",
    "        while True:\n",
    "            params = {\n",
    "                \"subreddit\": subreddit,\n",
    "                \"after\": after,\n",
    "                \"before\": current_before,\n",
    "                \"limit\": \"auto\",\n",
    "                \"sort\": \"desc\",\n",
    "            }\n",
    "            r = requests.get(BASE_URL_POSTS, params=params, timeout=60)\n",
    "            r.raise_for_status()\n",
    "            data = r.json()\n",
    "\n",
    "            # NEW: extract the list from the 'data' field\n",
    "            items = data.get(\"data\", []) if isinstance(data, dict) else data\n",
    "\n",
    "            if not isinstance(items, list) or not items:\n",
    "                break\n",
    "\n",
    "            for it in items:\n",
    "                f_out.write(json.dumps(it) + \"\\n\")\n",
    "\n",
    "            created_values = [it.get(\"created_utc\") for it in items if \"created_utc\" in it]\n",
    "            if not created_values:\n",
    "                break\n",
    "\n",
    "            oldest = min(created_values)\n",
    "            if not isinstance(oldest, (int, float)):\n",
    "                break\n",
    "            if oldest <= 0:\n",
    "                break\n",
    "\n",
    "            next_before_ts = oldest - 1\n",
    "            current_before = str(next_before_ts)\n",
    "            time.sleep(0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179e50bd",
   "metadata": {},
   "source": [
    ">Fetch comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec084feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_comments(subreddit: str, after: str, before: str, outfile: Path):\n",
    "    current_before = before\n",
    "\n",
    "    with outfile.open(\"w\", encoding=\"utf-8\") as f_out:\n",
    "        while True:\n",
    "            params = {\n",
    "                \"subreddit\": subreddit,\n",
    "                \"after\": after,\n",
    "                \"before\": current_before,\n",
    "                \"limit\": \"auto\",\n",
    "                \"sort\": \"desc\",\n",
    "            }\n",
    "            r = requests.get(BASE_URL_COMMENTS, params=params, timeout=60)\n",
    "            r.raise_for_status()\n",
    "            data = r.json()\n",
    "\n",
    "            items = data.get(\"data\", []) if isinstance(data, dict) else data\n",
    "\n",
    "            if not isinstance(items, list) or not items:\n",
    "                break\n",
    "\n",
    "            for it in items:\n",
    "                f_out.write(json.dumps(it) + \"\\n\")\n",
    "\n",
    "            created_values = [it.get(\"created_utc\") for it in items if \"created_utc\" in it]\n",
    "            if not created_values:\n",
    "                break\n",
    "\n",
    "            oldest = min(created_values)\n",
    "            if not isinstance(oldest, (int, float)):\n",
    "                break\n",
    "            if oldest <= 0:\n",
    "                break\n",
    "\n",
    "            next_before_ts = oldest - 1\n",
    "            current_before = str(next_before_ts)\n",
    "            time.sleep(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c097c99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_keywords(text: str) -> bool:\n",
    "    t = text.lower()\n",
    "    return any(k in t for k in KEYWORDS_LOWER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792194f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_filtered_dataset():\n",
    "    matched_posts = []\n",
    "    matched_post_ids = set()\n",
    "    matched_comments = []\n",
    "\n",
    "    # Submissions\n",
    "    for subreddit in SUBREDDITS:\n",
    "        sub_safe = subreddit.replace(\"/\", \"_\")\n",
    "        posts_path = OUT_DIR / f\"{sub_safe}_posts_2020_2025.jsonl\"\n",
    "        if not posts_path.exists():\n",
    "            continue\n",
    "\n",
    "        with posts_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                d = json.loads(line)\n",
    "                title = d.get(\"title\", \"\") or \"\"\n",
    "                body = d.get(\"selftext\", \"\") or \"\"\n",
    "                text = f\"{title} {body}\"\n",
    "                if match_keywords(text):\n",
    "                    post_id = d.get(\"id\")\n",
    "                    matched_posts.append(\n",
    "                        {\n",
    "                            \"type\": \"submission\",\n",
    "                            \"id\": post_id,\n",
    "                            \"link_id\": post_id,\n",
    "                            \"parent_id\": None,\n",
    "                            \"subreddit\": d.get(\"subreddit\"),\n",
    "                            \"title\": title,\n",
    "                            \"body\": body,\n",
    "                            \"created_utc\": d.get(\"created_utc\"),\n",
    "                            \"score\": d.get(\"score\"),\n",
    "                            \"num_comments\": d.get(\"num_comments\"),\n",
    "                        }\n",
    "                    )\n",
    "                    if post_id is not None:\n",
    "                        matched_post_ids.add(post_id)\n",
    "\n",
    "    # Comments\n",
    "    for subreddit in SUBREDDITS:\n",
    "        sub_safe = subreddit.replace(\"/\", \"_\")\n",
    "        comments_path = OUT_DIR / f\"{sub_safe}_comments_2020_2025.jsonl\"\n",
    "        if not comments_path.exists():\n",
    "            continue\n",
    "\n",
    "        with comments_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                d = json.loads(line)\n",
    "                body = d.get(\"body\", \"\") or \"\"\n",
    "                link_id = d.get(\"link_id\")\n",
    "                parent_id = d.get(\"parent_id\")\n",
    "\n",
    "                norm_link_id = link_id\n",
    "                if isinstance(link_id, str) and link_id.startswith(\"t3_\"):\n",
    "                    norm_link_id = link_id[3:]\n",
    "\n",
    "                keep = False\n",
    "                if match_keywords(body):\n",
    "                    keep = True\n",
    "                elif norm_link_id and norm_link_id in matched_post_ids:\n",
    "                    keep = True\n",
    "\n",
    "                if keep:\n",
    "                    matched_comments.append(\n",
    "                        {\n",
    "                            \"type\": \"comment\",\n",
    "                            \"id\": d.get(\"id\"),\n",
    "                            \"link_id\": norm_link_id,\n",
    "                            \"parent_id\": parent_id,\n",
    "                            \"subreddit\": d.get(\"subreddit\"),\n",
    "                            \"title\": None,\n",
    "                            \"body\": body,\n",
    "                            \"created_utc\": d.get(\"created_utc\"),\n",
    "                            \"score\": d.get(\"score\"),\n",
    "                            \"num_comments\": None,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "    df = pd.DataFrame(matched_posts + matched_comments)\n",
    "    out_name = (\n",
    "        \"reddit_inflation_2020_2025_posts_and_comments_TEST.parquet\"\n",
    "        if TEST_MODE\n",
    "        else \"reddit_inflation_2020_2025_posts_and_comments.parquet\"\n",
    "    )\n",
    "    df.to_parquet(BASE_PATH / out_name, engine=\"pyarrow\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45548ea5",
   "metadata": {},
   "source": [
    "### Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb9814d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(f\"TEST_MODE = {TEST_MODE}\")\n",
    "    print(\"Subreddits:\", SUBREDDITS)\n",
    "    print(\"Date range:\", AFTER_DATE, \"to\", BEFORE_DATE)\n",
    "    print(\"Base path:\", BASE_PATH)\n",
    "    print(\"Raw output dir:\", OUT_DIR)\n",
    "\n",
    "    for sub in SUBREDDITS:\n",
    "        print(f\"\\nDownloading posts for r/{sub}...\")\n",
    "        sub_safe = sub.replace(\"/\", \"_\")\n",
    "        posts_out = OUT_DIR / f\"{sub_safe}_posts_2020_2025.jsonl\"\n",
    "        comments_out = OUT_DIR / f\"{sub_safe}_comments_2020_2025.jsonl\"\n",
    "\n",
    "        fetch_posts(sub, AFTER_DATE, BEFORE_DATE, posts_out)\n",
    "        print(f\"Saved posts to {posts_out}\")\n",
    "\n",
    "        print(f\"Downloading comments for r/{sub}...\")\n",
    "        fetch_comments(sub, AFTER_DATE, BEFORE_DATE, comments_out)\n",
    "        print(f\"Saved comments to {comments_out}\")\n",
    "\n",
    "    print(\"\\nBuilding filtered dataset...\")\n",
    "    build_filtered_dataset()\n",
    "    print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipynb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
